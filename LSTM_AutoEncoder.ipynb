{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arksolutionzz/ark/blob/master/LSTM_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggJB0EDaSAh8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "max_len = 100  # Cut texts after this number of words\n",
        "\n",
        "# Load the data\n",
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "mx6bdXOBSCkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95c6595-b43b-4590-d912-7090d936de6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "latent_dim = 64\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(max_len,))\n",
        "embedding = Embedding(max_features, embedding_dim, input_length=max_len)(inputs)  # Embedding layer to convert words to vectors\n",
        "encoded = LSTM(latent_dim)(embedding)  # LSTM layer to encode the input sequence into a latent vector\n",
        "\n",
        "# Decoder\n",
        "decoded = RepeatVector(max_len)(encoded)  # RepeatVector repeats the encoded vector max_len times to match the input sequence length\n",
        "decoded = LSTM(embedding_dim, return_sequences=True)(decoded)  # LSTM layer to decode the repeated vector back into sequence form\n",
        "decoded = TimeDistributed(Dense(max_features, activation='softmax'))(decoded)  # TimeDistributed applies Dense layer to each time step\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  # Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1uVdVyVSLth",
        "outputId": "03f6c39d-a5eb-417a-f38f-059324a7f049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 128)          1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                49408     \n",
            "                                                                 \n",
            " repeat_vector (RepeatVecto  (None, 100, 64)           0         \n",
            " r)                                                              \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100, 128)          98816     \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 100, 10000)        1290000   \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2718224 (10.37 MB)\n",
            "Trainable params: 2718224 (10.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We need to expand the dimensions of the target data to match the output of the model\n",
        "x_train_exp = np.expand_dims(x_train, -1)\n",
        "x_test_exp = np.expand_dims(x_test, -1)"
      ],
      "metadata": {
        "id": "8G_EEV-KSQwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "autoencoder.fit(x_train, x_train_exp, epochs=10, batch_size=32, validation_data=(x_test, x_test_exp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzdjf-Z6SV3B",
        "outputId": "722a40a2-2a31-429b-c235-b8801bcdf50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 89s 99ms/step - loss: 6.4455 - val_loss: 6.3526\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 6.3616 - val_loss: 6.3448\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 6.3530 - val_loss: 6.3385\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 54s 69ms/step - loss: 7.3539 - val_loss: 7.0274\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 6.4360 - val_loss: 6.1805\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 50s 63ms/step - loss: 6.1854 - val_loss: 6.1598\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 6.1783 - val_loss: 6.1579\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 6.1760 - val_loss: 6.1641\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 6.1745 - val_loss: 6.1554\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 6.1731 - val_loss: 6.1597\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x782bf2b5b970>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = autoencoder.predict(x_test[:10])\n",
        "\n",
        "# Convert predictions to text for evaluation (this part is simplified and illustrative)\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(encoded_review):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
        "\n",
        "# Example of decoding original and reconstructed reviews\n",
        "original_review = decode_review(x_test[0])\n",
        "reconstructed_review = decode_review(np.argmax(predictions[0], axis=-1))\n",
        "\n",
        "print(f\"Original review: {original_review}\")\n",
        "print(f\"Reconstructed review: {reconstructed_review}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pspzisZSaZo",
        "outputId": "bacbdddc-bc62-4001-9111-b28fb78acb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "Original review: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? the wonder own as by is sequence i i and and to of hollywood br of down shouting getting boring of ever it sadly sadly sadly i i was then does don't close faint after one carry as by are be favourites all family turn in does as three part in another some to be probably with world and her an have faint beginning own as is sequence\n",
            "Reconstructed review: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? the the as as as of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n"
          ]
        }
      ]
    }
  ]
}